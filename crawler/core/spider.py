import os
from typing import List, Dict, Any
import scrapy
from scrapy.http import Request, FormRequest
from bs4 import BeautifulSoup
from PIL import Image
import pytesseract
from pdf2image import convert_from_path
from docx import Document
from pptx import Presentation
import pylibmagic
import magic
import requests
from pydantic import BaseModel, Field
from dotenv import load_dotenv

load_dotenv()

class KMSItem(BaseModel):
    title: str = Field(description="æ–‡æ¡£æ ‡é¢˜")
    content: str = Field(description="æ–‡æ¡£å†…å®¹")
    attachments: List[Dict[str, Any]] = Field(default_factory=list, description="é™„ä»¶ä¿¡æ¯")

class ConfluenceSpider(scrapy.Spider):
    name = 'confluence'
    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'COOKIES_ENABLED': True,
        'CONCURRENT_REQUESTS': 1
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_urls = [kwargs.get('start_url', 'http://kms.new-see.com:8090')]
        self.basic_auth = ('newsee', 'newsee')
        self.auth = {
            'os_username': 'zengdi',
            'os_password': '808611'
        }

        self.baichuan_config = {
            'api_key': os.getenv('BAI_CH_API_KEK'),
            'api_url': 'https://api.baichuan-ai.com/v1/chat/completions'
        }

    def start_requests(self):
        for url in self.start_urls:
            yield Request(
                url,
                callback=self.parse,
                meta={
                    'cookiejar': 1,
                    'dont_redirect': False
                },
                dont_filter=True
            )

    def parse(self, response):
        # å¤„ç†ç™»å½•
        if response.css('#loginform'):
            return self.login(response)

        # è§£æå¯¼èˆªæ ‘
        tree_links = response.css('.plugin-tabmeta-details a::attr(href)').getall()
        for link in tree_links:
            if 'pageId' in link:
                yield response.follow(link, callback=self.parse_content)

    def login(self, response):
        print("ğŸš€ ~ self, response:", self, response)

        res = FormRequest.from_response(
            response,
            formcss='#loginform',
            formdata={'os_username': self.auth['os_username'], 'os_password': self.auth['os_password']},
            clickdata={'css': '#loginButton'},
            callback=self.after_login,
            dont_filter=True,
            meta={'cookies': response.meta.get('cookies')}
        )
        print("ğŸš€ ~ self, response:", self, res)
        return res

    def after_login(self, response):
        if 'login' not in response.url:
            return self.parse(response)
        else:
            self.logger.error('ç™»å½•å¤±è´¥')
            return None

    def optimize_content(self, content: str) -> str:
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.baichuan_config["api_key"]}'
        }

        data = {
            'model': 'Baichuan4',
            'messages': [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¼˜åŒ–åŠ©æ‰‹ï¼Œéœ€è¦å¯¹è¾“å…¥çš„æ–‡æ¡£å†…å®¹è¿›è¡Œç»“æ„åŒ–å’Œä¼˜åŒ–å¤„ç†ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°æ˜“è¯»ï¼ŒåŒæ—¶ä¿æŒåŸæœ‰çš„æ ¸å¿ƒä¿¡æ¯å’Œä¸“ä¸šæ€§ã€‚'
                },
                {
                    'role': 'user',
                    'content': content
                }
            ],
            'temperature': 0.3,
            'stream': False
        }

        try:
            response = requests.post(self.baichuan_config['api_url'], headers=headers, json=data)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']
        except Exception as e:
            self.logger.error(f'ç™¾å·APIè°ƒç”¨å¤±è´¥: {str(e)}')
            return content

    def parse_content(self, response):
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.select_one('#title-text').get_text(strip=True)
        content = soup.select_one('#main-content')

        # å¤„ç†é™„ä»¶
        attachments = []
        for attachment in soup.select('.attachment-content'):
            file_url = attachment.select_one('a::attr(href)').get()
            file_type = magic.from_file(file_url, mime=True)

            if 'image' in file_type:
                text = self.process_image(file_url)
            elif 'pdf' in file_type:
                text = self.process_pdf(file_url)
            elif 'word' in file_type:
                text = self.process_word(file_url)
            elif 'powerpoint' in file_type:
                text = self.process_ppt(file_url)
            else:
                text = None

            if text:
                attachments.append({
                    'url': file_url,
                    'type': file_type,
                    'content': text
                })

        # ä½¿ç”¨ç™¾å·APIä¼˜åŒ–å†…å®¹
        optimized_content = self.optimize_content(content.get_text())

        yield KMSItem(
            title=title,
            content=optimized_content,
            attachments=attachments
        )

    @staticmethod
    def process_image(image_path):
        image = Image.open(image_path)
        return pytesseract.image_to_string(image, lang='chi_sim')

    @staticmethod
    def process_pdf(pdf_path):
        pages = convert_from_path(pdf_path)
        text = ''
        for page in pages:
            text += pytesseract.image_to_string(page, lang='chi_sim')
        return text

    @staticmethod
    def process_word(docx_path):
        doc = Document(docx_path)
        return '\n'.join([paragraph.text for paragraph in doc.paragraphs])

    @staticmethod
    def process_ppt(pptx_path):
        prs = Presentation(pptx_path)
        text = ''
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, 'text'):
                    text += shape.text + '\n'
        return text